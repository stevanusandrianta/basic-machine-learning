{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent\n",
    "\n",
    "In this exercise, i will develop a basic machine learning algorithm for regression problem which is Linear Regression.  We will compare the basic gradient descent and stochastic gradient descent.  \n",
    "On top of it, we will also try to implement a regularization namely L1 and L2 and see how this regularization will affect the ML model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we have x and y, where :  \n",
    "- x is the feature\n",
    "- y is the label\n",
    "\n",
    "to make things start really simple, we will just use python list and start with 1 feature and 1 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3])\n",
    "y = np.array([2.1, 3.2, 4.1])\n",
    "\n",
    "w0 = np.random.uniform(0,1)\n",
    "w1 = np.random.uniform(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "loss : 1.8473932702576625\n",
      "iteration number 200\n",
      "loss : 0.0011112237663111412\n",
      "iteration number 400\n",
      "loss : 0.0011111120045664508\n",
      "iteration number 600\n",
      "loss : 0.0011111111181970085\n",
      "iteration number 800\n",
      "loss : 0.0011111111111673055\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(epoch):\n",
    "    n = len(x)\n",
    "    #get prediction value\n",
    "    pred = w0 + w1*x\n",
    "    error = pred - y\n",
    "    loss = np.sum(error**2) / (2*n)\n",
    "    \n",
    "    delta_w0 = np.sum(error) / n\n",
    "    delta_w1 = np.sum(error * x) / n\n",
    "    \n",
    "    if not i%200:\n",
    "        print('iteration number {}'.format(i))\n",
    "        print('loss : {}'.format(loss))\n",
    "    \n",
    "    w0 -= learning_rate * delta_w0\n",
    "    w1 -= learning_rate * delta_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.13333338 3.13333334 4.13333331]\n",
      "[2.1 3.2 4.1]\n"
     ]
    }
   ],
   "source": [
    "print(w0 + w1*x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result are pretty close to our objective!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try the matrix version\n",
    "\n",
    "First let's initialize everything that we need  \n",
    "We will present X as a column vector and Y as a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "total_row = 100\n",
    "total_feature = 5\n",
    "mean = 100\n",
    "std = 10\n",
    "\n",
    "raw_x = np.vstack([np.random.normal(mean,std,total_row) for i in range(total_feature)]).T\n",
    "x = np.append(np.ones((total_row,1)),raw_x,axis=1)\n",
    "print(x.shape)\n",
    "\n",
    "y = np.random.normal(mean,std,total_row)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "loss : 41557.05513270729\n",
      "iteration number 200\n",
      "loss : 136.49534137179415\n",
      "iteration number 400\n",
      "loss : 107.15388264265215\n",
      "iteration number 600\n",
      "loss : 88.8465962499302\n",
      "iteration number 800\n",
      "loss : 77.36232552773578\n"
     ]
    }
   ],
   "source": [
    "w = np.random.normal(0,1,total_feature+1)\n",
    "# print(w.shape)\n",
    "\n",
    "epoch = 1000\n",
    "learning_rate = 0.00001\n",
    "\n",
    "for i in range(epoch):\n",
    "    n = len(x)\n",
    "    #get prediction value\n",
    "    pred = np.dot(x,w)\n",
    "    error = pred - y\n",
    "    loss = np.sum(error**2) / (2*n)\n",
    "    \n",
    "    delta_w = np.dot(error,x) / n\n",
    "    \n",
    "    if not i%200:\n",
    "        print('iteration number {}'.format(i))\n",
    "        print('loss : {}'.format(loss))\n",
    "    \n",
    "    w -= learning_rate * delta_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Iterating through all the example can be costly,  \n",
    "with SGD we only run through some example (randomizing the sample data), the number or sample we will call as a `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "loss : 7317.981902148507\n",
      "iteration number 200\n",
      "loss : 55.58489541738622\n",
      "iteration number 400\n",
      "loss : 47.70763872008232\n",
      "iteration number 600\n",
      "loss : 60.835224133588056\n",
      "iteration number 800\n",
      "loss : 47.78195444710167\n"
     ]
    }
   ],
   "source": [
    "sgd_w = np.random.normal(0,1,total_feature+1)\n",
    "# print(sgd_w.shape)\n",
    "\n",
    "epoch = 1000\n",
    "learning_rate = 0.00001\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(epoch):\n",
    "    n = len(x)\n",
    "    \n",
    "    sample_index = random.sample(range(n), batch_size)\n",
    "    sample_x = x[sample_index,:]\n",
    "    sample_y = y[sample_index]\n",
    "    \n",
    "    #get prediction value\n",
    "    pred = np.dot(sample_x,sgd_w)\n",
    "    error = pred - sample_y\n",
    "    loss = np.sum(error**2) / (2*n)\n",
    "    \n",
    "    delta_w = np.dot(error,sample_x) / n\n",
    "    \n",
    "    if not i%200:\n",
    "        print('iteration number {}'.format(i))\n",
    "        print('loss : {}'.format(loss))\n",
    "    \n",
    "    sgd_w -= learning_rate * delta_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Since everything went well, we will introduce a regularization L1 and L2  \n",
    "There is some good reading around here : https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261\n",
    "\n",
    "In order to implement some error, we will use different loss, which is sum square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "iteration number 0\n",
      "loss : 143755026.7819015\n",
      "iteration number 1000\n",
      "loss : 110253.22041986421\n",
      "iteration number 2000\n",
      "loss : 17135.920181149297\n",
      "iteration number 3000\n",
      "loss : 13601.384358595089\n",
      "iteration number 4000\n",
      "loss : 13401.579813652424\n",
      "iteration number 5000\n",
      "loss : 13387.598666296826\n",
      "iteration number 6000\n",
      "loss : 13386.362927453943\n",
      "iteration number 7000\n",
      "loss : 13386.066630217578\n",
      "iteration number 8000\n",
      "loss : 13385.841064640268\n",
      "iteration number 9000\n",
      "loss : 13385.620871860227\n"
     ]
    }
   ],
   "source": [
    "l_w = np.random.normal(0,10,total_feature+1)\n",
    "print(l_w.shape)\n",
    "\n",
    "#TRAINING TIME\n",
    "#THIS TRAINING WILL USE L0 or no regularization at all\n",
    "epoch = 10000\n",
    "learning_rate = 0.0000001\n",
    "regularization_rate = 0.0000001\n",
    "\n",
    "for i in range(epoch):\n",
    "    n = len(x)\n",
    "    #get prediction value\n",
    "    pred = np.dot(x,l_w)\n",
    "    error = pred - y\n",
    "    loss = np.sum(error**2)\n",
    "    \n",
    "    delta_w = np.dot(error,2*x)\n",
    "    \n",
    "    if not i%1000:\n",
    "        print('iteration number {}'.format(i))\n",
    "        print('loss : {}'.format(loss))\n",
    "    \n",
    "    l_w -= learning_rate * delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "loss : 418452007.38917416\n",
      "iteration number 1000\n",
      "loss : 178425.7736440261\n",
      "iteration number 2000\n",
      "loss : 22838.58720822707\n",
      "iteration number 3000\n",
      "loss : 13698.292235264465\n",
      "iteration number 4000\n",
      "loss : 13066.251622974813\n",
      "iteration number 5000\n",
      "loss : 13019.639466495992\n",
      "iteration number 6000\n",
      "loss : 13015.967203419077\n",
      "iteration number 7000\n",
      "loss : 13015.51601185989\n",
      "iteration number 8000\n",
      "loss : 13015.308307826881\n",
      "iteration number 9000\n",
      "loss : 13015.119070547104\n"
     ]
    }
   ],
   "source": [
    "l_one_w = np.random.normal(0,10,total_feature+1)\n",
    "\n",
    "#TRAINING TIME\n",
    "#THIS TRAINING WILL USE L1 \n",
    "epoch = 10000\n",
    "learning_rate = 0.0000001\n",
    "regularization_rate = 0.001\n",
    "\n",
    "for i in range(epoch):\n",
    "    n = len(x)\n",
    "    #get prediction value\n",
    "    pred = np.dot(x,l_one_w)\n",
    "    error = pred - y\n",
    "    loss = np.sum(error**2) + np.sum(regularization_rate * np.absolute(l_one_w))\n",
    "    \n",
    "    delta_w = np.dot(error,2*x) + (np.array([1 if ww>0 else -1 for ww in l_one_w]) * regularization_rate)\n",
    "    \n",
    "    if not i%1000:\n",
    "        print('iteration number {}'.format(i))\n",
    "        print('loss : {}'.format(loss))\n",
    "    \n",
    "    l_one_w -= learning_rate * delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "loss : 2591491458.6033354\n",
      "iteration number 1000\n",
      "loss : 53645.83481158214\n",
      "iteration number 2000\n",
      "loss : 15363.293446707274\n",
      "iteration number 3000\n",
      "loss : 13107.042188666705\n",
      "iteration number 4000\n",
      "loss : 12948.513804834893\n",
      "iteration number 5000\n",
      "loss : 12936.580636088425\n",
      "iteration number 6000\n",
      "loss : 12935.513609425416\n",
      "iteration number 7000\n",
      "loss : 12935.2657647719\n",
      "iteration number 8000\n",
      "loss : 12935.079981168486\n",
      "iteration number 9000\n",
      "loss : 12934.898918806608\n"
     ]
    }
   ],
   "source": [
    "l_two_w = np.random.normal(0,10,total_feature+1)\n",
    "\n",
    "#TRAINING TIME\n",
    "#THIS TRAINING WILL USE L1 \n",
    "epoch = 10000\n",
    "learning_rate = 0.0000001\n",
    "regularization_rate = 0.001\n",
    "\n",
    "for i in range(epoch):\n",
    "    n = len(x)\n",
    "    pred = np.dot(x,l_two_w)\n",
    "    error = pred - y\n",
    "    loss = np.sum(error**2) + np.sum(regularization_rate * l_two_w**2)\n",
    "    \n",
    "    delta_w = np.dot(error,2*x) + (2*regularization_rate*l_two_w)\n",
    "    \n",
    "    if not i%1000:\n",
    "        print('iteration number {}'.format(i))\n",
    "        print('loss : {}'.format(loss))\n",
    "    \n",
    "    l_two_w -= learning_rate * delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.09942194,  0.22189387,  0.16570789,  0.11135865,  0.18739065,\n",
       "        0.32159271])"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1150334 , 0.20889359, 0.14450967, 0.09459891, 0.17643099,\n",
       "       0.30179577])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_one_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.984858  , 0.20591197, 0.13968926, 0.09078336, 0.17389809,\n",
       "       0.29734472])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_two_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Here i learn how to build a regression and linear regression,\n",
    "The difference between GD and SGD is SGD not using all the dataset to calculate the loss function, thus the calculate will be faster and yield similar result as well\n",
    "\n",
    "about Regularization, i read that it is important, but can not see the result in this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading List\n",
    "\n",
    "- https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
