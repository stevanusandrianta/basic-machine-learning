{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Gradient Descent\n",
    "\n",
    "This is continuing the machine learning topic, but now we are talking about classification problem instead of regression problem.  \n",
    "I will not cover about matrices dataset, since the ideas is exactly the same as written in the previous notebook\n",
    "\n",
    "Here is a very good link to learn about Logistic Regression : https://towardsdatascience.com/logistic-regression-from-very-scratch-ea914961f320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "y = np.array([0,0,0,1,1,1,1,0,1,1])\n",
    "\n",
    "w0 = np.random.uniform(0,1)\n",
    "w1 = np.random.uniform(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(d):\n",
    "    return 1/(1+np.exp(-d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "loss : 0.9329497810135313\n",
      "iteration number 200\n",
      "loss : 0.5237210995163079\n",
      "iteration number 400\n",
      "loss : 0.49763962849376997\n",
      "iteration number 600\n",
      "loss : 0.49207592733533706\n",
      "iteration number 800\n",
      "loss : 0.4906751592034265\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(epoch):\n",
    "    n = len(x)\n",
    "    z = w0 + w1*x\n",
    "    pred = sigmoid(z)\n",
    "    error = pred - y\n",
    "    loss = np.sum(-1 * (y * np.log(pred) + (1-y)*(np.log(1-pred))))/n\n",
    "    \n",
    "    delta_w0 = np.sum(error) / n\n",
    "    delta_w1 = np.sum(x * error) / n\n",
    "    \n",
    "    if not i%200:\n",
    "        print('iteration number {}'.format(i))\n",
    "        print('loss : {}'.format(loss))\n",
    "    \n",
    "    w0 -= learning_rate * delta_w0\n",
    "    w1 -= learning_rate * delta_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15713405, 0.23708398, 0.34124333, 0.46336992, 0.59005477,\n",
       "       0.70581968, 0.79997562, 0.86956489, 0.9174423 , 0.94878099])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w0 + w1*x\n",
    "pred = sigmoid(z)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
